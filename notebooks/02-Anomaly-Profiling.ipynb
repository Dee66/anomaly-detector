{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1dd5d61",
   "metadata": {},
   "source": [
    "# üìä Anomaly Profiling for Security Event Analysis\n",
    "\n",
    "This notebook provides comprehensive analysis and profiling of anomalies detected in security logs. It helps security analysts understand anomaly patterns, validate detection algorithms, and tune anomaly scoring thresholds.\n",
    "\n",
    "## Anomaly Types\n",
    "- **Statistical Outliers**: Events that deviate significantly from normal patterns\n",
    "- **Behavioral Deviations**: Unusual user/entity behavior compared to historical baselines\n",
    "- **Rare Entity Combinations**: Uncommon combinations of entities in security events\n",
    "- **Time Pattern Anomalies**: Events occurring at unusual times for specific entities\n",
    "- **Privilege Escalation**: Patterns indicating potential privilege abuse\n",
    "- **Lateral Movement**: Patterns suggesting unauthorized access expansion\n",
    "\n",
    "## Analysis Features\n",
    "- Real-time anomaly scoring and visualization\n",
    "- Historical trend analysis\n",
    "- Entity behavior profiling\n",
    "- False positive analysis\n",
    "- Threshold optimization\n",
    "- Security incident correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbbaa93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML, clear_output\n",
    "from datetime import datetime, timedelta\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Import project modules\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from src.detector.scoring import (\n",
    "    AnomalyScoringEngine, StatisticalAnomalyDetector, BehavioralAnomalyDetector,\n",
    "    RareEntityCombinationDetector, PrivilegeEscalationDetector, AnomalyType\n",
    ")\n",
    "from src.detector.schemas import CloudTrailEvent, ExtractedEntity, AnomalyScore\n",
    "from src.detector.core import EnrichmentEngine\n",
    "\n",
    "print(\"üìä Anomaly Profiling System loaded successfully!\")\n",
    "print(\"üîß Available anomaly detectors ready for analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eabbd12",
   "metadata": {},
   "source": [
    "## üîß Anomaly Detection Engine Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709c0811",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnomalyProfiler:\n",
    "    \"\"\"Comprehensive anomaly profiling and analysis tool.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the anomaly profiler.\"\"\"\n",
    "        # Initialize anomaly scoring engine\n",
    "        self.config = {\n",
    "            'anomaly_threshold': 5.0,\n",
    "            'statistical_threshold': 2.5\n",
    "        }\n",
    "        self.scoring_engine = AnomalyScoringEngine(self.config)\n",
    "        \n",
    "        # Data storage\n",
    "        self.events_data = []\n",
    "        self.anomalies_data = []\n",
    "        self.analysis_results = {}\n",
    "        \n",
    "        # Visualization settings\n",
    "        self.color_map = {\n",
    "            'STATISTICAL_OUTLIER': '#FF6B6B',\n",
    "            'RARE_ENTITY_COMBO': '#4ECDC4',\n",
    "            'BEHAVIORAL_DEVIATION': '#45B7D1',\n",
    "            'TIME_PATTERN_ANOMALY': '#96CEB4',\n",
    "            'PRIVILEGE_ESCALATION': '#FFEAA7',\n",
    "            'LATERAL_MOVEMENT': '#DDA0DD'\n",
    "        }\n",
    "    \n",
    "    def load_synthetic_data(self, num_events: int = 1000) -> None:\n",
    "        \"\"\"Load synthetic security events for analysis.\"\"\"\n",
    "        print(f\"üîÑ Generating {num_events} synthetic security events...\")\n",
    "        \n",
    "        # Generate synthetic events with varying patterns\n",
    "        base_time = datetime.now() - timedelta(days=30)\n",
    "        \n",
    "        users = ['alice.smith', 'bob.jones', 'carol.white', 'david.brown', 'eve.wilson']\n",
    "        roles = ['AdminRole', 'PowerUserRole', 'ReadOnlyRole', 'S3FullAccessRole']\n",
    "        actions = ['AssumeRole', 'CreateUser', 'AttachUserPolicy', 'ListUsers', 'DescribeInstances', \n",
    "                  'CreateRole', 'DeleteUser', 'CreateAccessKey', 'GetCredentialsForIdentity']\n",
    "        ips = ['192.168.1.100', '10.0.1.50', '172.16.0.25', '203.0.113.1', '198.51.100.1']\n",
    "        regions = ['us-east-1', 'us-west-2', 'eu-west-1', 'ap-southeast-1']\n",
    "        \n",
    "        events = []\n",
    "        for i in range(num_events):\n",
    "            # Create time patterns (some users more active during business hours)\n",
    "            if np.random.random() < 0.7:  # 70% business hours\n",
    "                hour_offset = np.random.randint(8, 18)  # 8 AM to 6 PM\n",
    "            else:  # 30% off hours (potential anomalies)\n",
    "                hour_offset = np.random.choice([0, 1, 2, 22, 23])  # Late night/early morning\n",
    "            \n",
    "            event_time = base_time + timedelta(\n",
    "                days=np.random.randint(0, 30),\n",
    "                hours=hour_offset,\n",
    "                minutes=np.random.randint(0, 60)\n",
    "            )\n",
    "            \n",
    "            # Introduce some anomalous patterns\n",
    "            if np.random.random() < 0.1:  # 10% privilege escalation patterns\n",
    "                user = np.random.choice(users[:2])  # Limit to first 2 users\n",
    "                action = np.random.choice(['CreateRole', 'AttachUserPolicy', 'AssumeRole'])\n",
    "            elif np.random.random() < 0.05:  # 5% rare combinations\n",
    "                user = 'system.automated'  # Unusual user\n",
    "                action = np.random.choice(actions)\n",
    "            else:  # Normal patterns\n",
    "                user = np.random.choice(users)\n",
    "                action = np.random.choice(actions)\n",
    "            \n",
    "            # Create event\n",
    "            from src.detector.schemas import UserIdentity\n",
    "            \n",
    "            event = CloudTrailEvent(\n",
    "                eventTime=event_time,\n",
    "                eventName=action,\n",
    "                userIdentity=UserIdentity(\n",
    "                    type=\"IAMUser\",\n",
    "                    principalId=f\"AIDACKCEVSQ6C2{i:06d}\",\n",
    "                    arn=f\"arn:aws:iam::123456789012:user/{user}\",\n",
    "                    accountId=\"123456789012\",\n",
    "                    userName=user\n",
    "                ),\n",
    "                awsRegion=np.random.choice(regions),\n",
    "                sourceIPAddress=np.random.choice(ips),\n",
    "                userAgent=\"aws-cli/2.0.0\",\n",
    "                requestParameters={},\n",
    "                responseElements={}\n",
    "            )\n",
    "            \n",
    "            # Create corresponding entities\n",
    "            entities = [\n",
    "                ExtractedEntity(\n",
    "                    entity_id=user,\n",
    "                    entity_type=\"USER\",\n",
    "                    context={\"source\": \"userIdentity\"},\n",
    "                    confidence=0.9\n",
    "                ),\n",
    "                ExtractedEntity(\n",
    "                    entity_id=event.sourceIPAddress,\n",
    "                    entity_type=\"IP\",\n",
    "                    context={\"source\": \"sourceIPAddress\"},\n",
    "                    confidence=0.95\n",
    "                )\n",
    "            ]\n",
    "            \n",
    "            # Add role entity if relevant\n",
    "            if 'Role' in action:\n",
    "                entities.append(ExtractedEntity(\n",
    "                    entity_id=np.random.choice(roles),\n",
    "                    entity_type=\"ROLE\",\n",
    "                    context={\"source\": \"requestParameters\"},\n",
    "                    confidence=0.85\n",
    "                ))\n",
    "            \n",
    "            events.append((event, entities))\n",
    "        \n",
    "        self.events_data = events\n",
    "        print(f\"‚úÖ Generated {len(events)} synthetic events\")\n",
    "    \n",
    "    def analyze_anomalies(self) -> None:\n",
    "        \"\"\"Analyze events for anomalies and collect results.\"\"\"\n",
    "        if not self.events_data:\n",
    "            print(\"‚ùå No events loaded. Please load data first.\")\n",
    "            return\n",
    "        \n",
    "        print(\"üîç Analyzing events for anomalies...\")\n",
    "        \n",
    "        anomalies = []\n",
    "        normal_events = []\n",
    "        \n",
    "        for i, (event, entities) in enumerate(self.events_data):\n",
    "            # Score the event\n",
    "            anomaly_scores = self.scoring_engine.score_event(event, entities)\n",
    "            \n",
    "            # Calculate aggregate score\n",
    "            aggregate_score = self.scoring_engine.calculate_aggregate_score(anomaly_scores)\n",
    "            \n",
    "            event_data = {\n",
    "                'index': i,\n",
    "                'timestamp': event.eventTime,\n",
    "                'user': event.userIdentity.userName,\n",
    "                'action': event.eventName,\n",
    "                'source_ip': event.sourceIPAddress,\n",
    "                'region': event.awsRegion,\n",
    "                'aggregate_score': aggregate_score,\n",
    "                'anomaly_scores': anomaly_scores,\n",
    "                'entities': entities,\n",
    "                'event': event\n",
    "            }\n",
    "            \n",
    "            if aggregate_score >= self.config['anomaly_threshold']:\n",
    "                anomalies.append(event_data)\n",
    "            else:\n",
    "                normal_events.append(event_data)\n",
    "        \n",
    "        self.anomalies_data = anomalies\n",
    "        self.normal_events = normal_events\n",
    "        \n",
    "        print(f\"üìä Analysis complete:\")\n",
    "        print(f\"  ‚Ä¢ Total events: {len(self.events_data)}\")\n",
    "        print(f\"  ‚Ä¢ Anomalies detected: {len(anomalies)} ({len(anomalies)/len(self.events_data)*100:.1f}%)\")\n",
    "        print(f\"  ‚Ä¢ Normal events: {len(normal_events)} ({len(normal_events)/len(self.events_data)*100:.1f}%)\")\n",
    "    \n",
    "    def create_anomaly_summary_df(self) -> pd.DataFrame:\n",
    "        \"\"\"Create a summary DataFrame of anomalies.\"\"\"\n",
    "        if not self.anomalies_data:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        summary_data = []\n",
    "        for anomaly in self.anomalies_data:\n",
    "            # Extract anomaly types\n",
    "            anomaly_types = []\n",
    "            factors = []\n",
    "            max_score = 0\n",
    "            \n",
    "            for score in anomaly['anomaly_scores']:\n",
    "                if score.is_anomaly:\n",
    "                    # This is a placeholder - the actual AnomalyResult would have type info\n",
    "                    factors.extend(score.factors)\n",
    "                    if score.score > max_score:\n",
    "                        max_score = score.score\n",
    "            \n",
    "            summary_data.append({\n",
    "                'timestamp': anomaly['timestamp'],\n",
    "                'user': anomaly['user'],\n",
    "                'action': anomaly['action'],\n",
    "                'source_ip': anomaly['source_ip'],\n",
    "                'region': anomaly['region'],\n",
    "                'aggregate_score': anomaly['aggregate_score'],\n",
    "                'max_individual_score': max_score,\n",
    "                'num_anomaly_factors': len(factors),\n",
    "                'factors': '; '.join(factors[:3]) if factors else 'Unknown',\n",
    "                'hour': anomaly['timestamp'].hour,\n",
    "                'day_of_week': anomaly['timestamp'].strftime('%A')\n",
    "            })\n",
    "        \n",
    "        return pd.DataFrame(summary_data)\n",
    "    \n",
    "    def plot_anomaly_timeline(self) -> None:\n",
    "        \"\"\"Plot anomaly detection timeline.\"\"\"\n",
    "        if not self.anomalies_data:\n",
    "            print(\"‚ùå No anomalies to plot\")\n",
    "            return\n",
    "        \n",
    "        df = self.create_anomaly_summary_df()\n",
    "        \n",
    "        fig = make_subplots(\n",
    "            rows=2, cols=1,\n",
    "            subplot_titles=['Anomaly Score Timeline', 'Anomaly Count by Hour'],\n",
    "            specs=[[{\"secondary_y\": False}], [{\"secondary_y\": False}]]\n",
    "        )\n",
    "        \n",
    "        # Timeline plot\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=df['timestamp'],\n",
    "                y=df['aggregate_score'],\n",
    "                mode='markers',\n",
    "                marker=dict(\n",
    "                    size=8,\n",
    "                    color=df['aggregate_score'],\n",
    "                    colorscale='Reds',\n",
    "                    showscale=True,\n",
    "                    colorbar=dict(title=\"Anomaly Score\")\n",
    "                ),\n",
    "                text=df['factors'],\n",
    "                hovertemplate='<b>%{text}</b><br>' +\n",
    "                             'Time: %{x}<br>' +\n",
    "                             'Score: %{y:.2f}<br>' +\n",
    "                             '<extra></extra>',\n",
    "                name='Anomalies'\n",
    "            ),\n",
    "            row=1, col=1\n",
    "        )\n",
    "        \n",
    "        # Hourly distribution\n",
    "        hourly_counts = df.groupby('hour').size().reset_index(name='count')\n",
    "        fig.add_trace(\n",
    "            go.Bar(\n",
    "                x=hourly_counts['hour'],\n",
    "                y=hourly_counts['count'],\n",
    "                marker_color='lightcoral',\n",
    "                name='Hourly Anomalies'\n",
    "            ),\n",
    "            row=2, col=1\n",
    "        )\n",
    "        \n",
    "        fig.update_layout(\n",
    "            height=800,\n",
    "            title_text=\"Anomaly Detection Analysis\",\n",
    "            showlegend=False\n",
    "        )\n",
    "        \n",
    "        fig.update_xaxes(title_text=\"Time\", row=1, col=1)\n",
    "        fig.update_yaxes(title_text=\"Anomaly Score\", row=1, col=1)\n",
    "        fig.update_xaxes(title_text=\"Hour of Day\", row=2, col=1)\n",
    "        fig.update_yaxes(title_text=\"Number of Anomalies\", row=2, col=1)\n",
    "        \n",
    "        fig.show()\n",
    "    \n",
    "    def plot_user_behavior_analysis(self) -> None:\n",
    "        \"\"\"Analyze and plot user behavior patterns.\"\"\"\n",
    "        if not self.anomalies_data:\n",
    "            print(\"‚ùå No anomalies to analyze\")\n",
    "            return\n",
    "        \n",
    "        df = self.create_anomaly_summary_df()\n",
    "        \n",
    "        fig = make_subplots(\n",
    "            rows=2, cols=2,\n",
    "            subplot_titles=[\n",
    "                'Anomalies by User', 'Anomalies by Action',\n",
    "                'Anomalies by Source IP', 'Score Distribution'\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # User analysis\n",
    "        user_counts = df.groupby('user').agg({\n",
    "            'aggregate_score': ['count', 'mean']\n",
    "        }).round(2)\n",
    "        user_counts.columns = ['count', 'avg_score']\n",
    "        user_counts = user_counts.reset_index().sort_values('count', ascending=False)\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Bar(x=user_counts['user'], y=user_counts['count'], name='User Anomalies'),\n",
    "            row=1, col=1\n",
    "        )\n",
    "        \n",
    "        # Action analysis\n",
    "        action_counts = df['action'].value_counts().head(10)\n",
    "        fig.add_trace(\n",
    "            go.Bar(x=action_counts.index, y=action_counts.values, name='Action Anomalies'),\n",
    "            row=1, col=2\n",
    "        )\n",
    "        \n",
    "        # IP analysis\n",
    "        ip_counts = df['source_ip'].value_counts().head(10)\n",
    "        fig.add_trace(\n",
    "            go.Bar(x=ip_counts.index, y=ip_counts.values, name='IP Anomalies'),\n",
    "            row=2, col=1\n",
    "        )\n",
    "        \n",
    "        # Score distribution\n",
    "        fig.add_trace(\n",
    "            go.Histogram(x=df['aggregate_score'], nbinsx=20, name='Score Distribution'),\n",
    "            row=2, col=2\n",
    "        )\n",
    "        \n",
    "        fig.update_layout(height=800, title_text=\"User Behavior Analysis\", showlegend=False)\n",
    "        fig.show()\n",
    "        \n",
    "        # Print top anomalous users\n",
    "        print(\"\\nüîç Top Anomalous Users:\")\n",
    "        for _, row in user_counts.head().iterrows():\n",
    "            print(f\"  ‚Ä¢ {row['user']}: {row['count']} anomalies (avg score: {row['avg_score']:.2f})\")\n",
    "\n",
    "# Initialize the profiler\n",
    "profiler = AnomalyProfiler()\n",
    "print(\"üìä Anomaly Profiler initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d2b37c",
   "metadata": {},
   "source": [
    "## üìà Data Loading and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763e1a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load synthetic data\n",
    "profiler.load_synthetic_data(num_events=2000)\n",
    "\n",
    "# Analyze for anomalies\n",
    "profiler.analyze_anomalies()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52969a6",
   "metadata": {},
   "source": [
    "## üìä Anomaly Visualization and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c41e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display anomaly timeline\n",
    "profiler.plot_anomaly_timeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e6c3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User behavior analysis\n",
    "profiler.plot_user_behavior_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f22a46",
   "metadata": {},
   "source": [
    "## üìã Detailed Anomaly Investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e225e5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create detailed anomaly summary\n",
    "anomaly_df = profiler.create_anomaly_summary_df()\n",
    "\n",
    "if not anomaly_df.empty:\n",
    "    print(\"üîç Top 10 Highest Scoring Anomalies:\")\n",
    "    top_anomalies = anomaly_df.nlargest(10, 'aggregate_score')\n",
    "    \n",
    "    display(top_anomalies[[\n",
    "        'timestamp', 'user', 'action', 'source_ip', \n",
    "        'aggregate_score', 'factors'\n",
    "    ]].style.format({\n",
    "        'aggregate_score': '{:.2f}',\n",
    "        'timestamp': lambda x: x.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    }).background_gradient(subset=['aggregate_score'], cmap='Reds'))\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(f\"\\nüìä Anomaly Statistics:\")\n",
    "    print(f\"  ‚Ä¢ Mean anomaly score: {anomaly_df['aggregate_score'].mean():.2f}\")\n",
    "    print(f\"  ‚Ä¢ Median anomaly score: {anomaly_df['aggregate_score'].median():.2f}\")\n",
    "    print(f\"  ‚Ä¢ Highest anomaly score: {anomaly_df['aggregate_score'].max():.2f}\")\n",
    "    print(f\"  ‚Ä¢ Most anomalous user: {anomaly_df['user'].value_counts().index[0]}\")\n",
    "    print(f\"  ‚Ä¢ Most anomalous action: {anomaly_df['action'].value_counts().index[0]}\")\n",
    "else:\n",
    "    print(\"‚ùå No anomalies detected in the current dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c2eac0",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Threshold Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e81cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_threshold_impact():\n",
    "    \"\"\"Analyze the impact of different anomaly thresholds.\"\"\"\n",
    "    if not profiler.events_data:\n",
    "        print(\"‚ùå No events data available\")\n",
    "        return\n",
    "    \n",
    "    thresholds = np.arange(1.0, 10.0, 0.5)\n",
    "    results = []\n",
    "    \n",
    "    print(\"üîÑ Analyzing threshold impact...\")\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        anomaly_count = 0\n",
    "        total_score = 0\n",
    "        \n",
    "        for event, entities in profiler.events_data:\n",
    "            anomaly_scores = profiler.scoring_engine.score_event(event, entities)\n",
    "            aggregate_score = profiler.scoring_engine.calculate_aggregate_score(anomaly_scores)\n",
    "            total_score += aggregate_score\n",
    "            \n",
    "            if aggregate_score >= threshold:\n",
    "                anomaly_count += 1\n",
    "        \n",
    "        anomaly_rate = anomaly_count / len(profiler.events_data)\n",
    "        avg_score = total_score / len(profiler.events_data)\n",
    "        \n",
    "        results.append({\n",
    "            'threshold': threshold,\n",
    "            'anomaly_count': anomaly_count,\n",
    "            'anomaly_rate': anomaly_rate,\n",
    "            'avg_score': avg_score\n",
    "        })\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Plot threshold analysis\n",
    "    fig = make_subplots(\n",
    "        rows=1, cols=2,\n",
    "        subplot_titles=['Anomaly Rate vs Threshold', 'Anomaly Count vs Threshold']\n",
    "    )\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=results_df['threshold'],\n",
    "            y=results_df['anomaly_rate'] * 100,\n",
    "            mode='lines+markers',\n",
    "            name='Anomaly Rate (%)',\n",
    "            line=dict(color='red')\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=results_df['threshold'],\n",
    "            y=results_df['anomaly_count'],\n",
    "            mode='lines+markers',\n",
    "            name='Anomaly Count',\n",
    "            line=dict(color='blue')\n",
    "        ),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    # Add current threshold line\n",
    "    current_threshold = profiler.config['anomaly_threshold']\n",
    "    fig.add_vline(x=current_threshold, line_dash=\"dash\", line_color=\"green\", \n",
    "                  annotation_text=f\"Current: {current_threshold}\", row=1, col=1)\n",
    "    fig.add_vline(x=current_threshold, line_dash=\"dash\", line_color=\"green\", \n",
    "                  annotation_text=f\"Current: {current_threshold}\", row=1, col=2)\n",
    "    \n",
    "    fig.update_layout(height=400, title_text=\"Threshold Optimization Analysis\")\n",
    "    fig.update_xaxes(title_text=\"Threshold\")\n",
    "    fig.update_yaxes(title_text=\"Anomaly Rate (%)\", row=1, col=1)\n",
    "    fig.update_yaxes(title_text=\"Number of Anomalies\", row=1, col=2)\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "    # Recommendations\n",
    "    print(\"\\nüí° Threshold Recommendations:\")\n",
    "    \n",
    "    # Find threshold that gives ~1-5% anomaly rate\n",
    "    target_rate = 0.02  # 2%\n",
    "    closest_idx = np.abs(results_df['anomaly_rate'] - target_rate).idxmin()\n",
    "    recommended_threshold = results_df.loc[closest_idx, 'threshold']\n",
    "    recommended_rate = results_df.loc[closest_idx, 'anomaly_rate']\n",
    "    \n",
    "    print(f\"  ‚Ä¢ For ~2% anomaly rate: threshold = {recommended_threshold:.1f} ({recommended_rate*100:.1f}% actual)\")\n",
    "    \n",
    "    # Current threshold performance\n",
    "    current_idx = np.abs(results_df['threshold'] - current_threshold).idxmin()\n",
    "    current_rate = results_df.loc[current_idx, 'anomaly_rate']\n",
    "    current_count = results_df.loc[current_idx, 'anomaly_count']\n",
    "    \n",
    "    print(f\"  ‚Ä¢ Current threshold {current_threshold}: {current_rate*100:.1f}% rate ({current_count} anomalies)\")\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "# Run threshold analysis\n",
    "threshold_results = analyze_threshold_impact()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0649f4df",
   "metadata": {},
   "source": [
    "## üéØ Interactive Anomaly Investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e747ce07",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InteractiveAnomalyInvestigator:\n",
    "    \"\"\"Interactive widget for investigating specific anomalies.\"\"\"\n",
    "    \n",
    "    def __init__(self, profiler: AnomalyProfiler):\n",
    "        self.profiler = profiler\n",
    "        self.current_anomaly_idx = 0\n",
    "        \n",
    "        if not self.profiler.anomalies_data:\n",
    "            print(\"‚ùå No anomalies available for investigation\")\n",
    "            return\n",
    "        \n",
    "        # Create widgets\n",
    "        self.anomaly_selector = widgets.IntSlider(\n",
    "            value=0,\n",
    "            min=0,\n",
    "            max=len(self.profiler.anomalies_data) - 1,\n",
    "            description='Anomaly #:',\n",
    "            style={'description_width': 'initial'}\n",
    "        )\n",
    "        \n",
    "        self.details_output = widgets.Output()\n",
    "        \n",
    "        # Connect events\n",
    "        self.anomaly_selector.observe(self.on_anomaly_change, names='value')\n",
    "        \n",
    "        # Layout\n",
    "        self.widget = widgets.VBox([\n",
    "            widgets.HTML(\"<h3>üîç Interactive Anomaly Investigation</h3>\"),\n",
    "            self.anomaly_selector,\n",
    "            self.details_output\n",
    "        ])\n",
    "        \n",
    "        # Show initial anomaly\n",
    "        self.show_anomaly_details(0)\n",
    "    \n",
    "    def on_anomaly_change(self, change):\n",
    "        \"\"\"Handle anomaly selection change.\"\"\"\n",
    "        self.show_anomaly_details(change['new'])\n",
    "    \n",
    "    def show_anomaly_details(self, idx: int):\n",
    "        \"\"\"Show detailed information about a specific anomaly.\"\"\"\n",
    "        with self.details_output:\n",
    "            clear_output()\n",
    "            \n",
    "            if idx >= len(self.profiler.anomalies_data):\n",
    "                print(\"‚ùå Invalid anomaly index\")\n",
    "                return\n",
    "            \n",
    "            anomaly = self.profiler.anomalies_data[idx]\n",
    "            event = anomaly['event']\n",
    "            \n",
    "            print(f\"üìã Anomaly #{idx + 1} Details\")\n",
    "            print(f\"{'='*50}\")\n",
    "            print(f\"‚è∞ Timestamp: {event.eventTime.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "            print(f\"üë§ User: {event.userIdentity.userName}\")\n",
    "            print(f\"üéØ Action: {event.eventName}\")\n",
    "            print(f\"üåê Source IP: {event.sourceIPAddress}\")\n",
    "            print(f\"üìç Region: {event.awsRegion}\")\n",
    "            print(f\"üìä Aggregate Score: {anomaly['aggregate_score']:.2f}\")\n",
    "            \n",
    "            print(f\"\\nüè∑Ô∏è Extracted Entities:\")\n",
    "            for entity in anomaly['entities']:\n",
    "                print(f\"  ‚Ä¢ {entity.entity_type}: {entity.entity_id} (confidence: {entity.confidence:.2f})\")\n",
    "            \n",
    "            print(f\"\\nüö® Anomaly Factors:\")\n",
    "            for i, score in enumerate(anomaly['anomaly_scores']):\n",
    "                if score.is_anomaly:\n",
    "                    print(f\"  {i+1}. Score: {score.score:.2f}\")\n",
    "                    for factor in score.factors:\n",
    "                        print(f\"     ‚Ä¢ {factor}\")\n",
    "            \n",
    "            # Contextual information\n",
    "            print(f\"\\nüìà Contextual Analysis:\")\n",
    "            \n",
    "            # Time context\n",
    "            hour = event.eventTime.hour\n",
    "            if hour < 6 or hour > 20:\n",
    "                print(f\"  ‚ö†Ô∏è Unusual time: {hour:02d}:xx (outside business hours)\")\n",
    "            else:\n",
    "                print(f\"  ‚úÖ Normal time: {hour:02d}:xx (business hours)\")\n",
    "            \n",
    "            # User context\n",
    "            user_anomalies = [a for a in self.profiler.anomalies_data if a['user'] == anomaly['user']]\n",
    "            print(f\"  üë§ User has {len(user_anomalies)} total anomalies\")\n",
    "            \n",
    "            # Action context\n",
    "            action_anomalies = [a for a in self.profiler.anomalies_data if a['action'] == anomaly['action']]\n",
    "            print(f\"  üéØ Action '{event.eventName}' appears in {len(action_anomalies)} anomalies\")\n",
    "            \n",
    "            # Investigation recommendations\n",
    "            print(f\"\\nüí° Investigation Recommendations:\")\n",
    "            if anomaly['aggregate_score'] > 8.0:\n",
    "                print(f\"  üî¥ HIGH PRIORITY: Immediate investigation recommended\")\n",
    "                print(f\"  üìû Consider contacting user: {event.userIdentity.userName}\")\n",
    "                print(f\"  üîí Review account permissions and recent changes\")\n",
    "            elif anomaly['aggregate_score'] > 6.0:\n",
    "                print(f\"  üü° MEDIUM PRIORITY: Monitor for additional suspicious activity\")\n",
    "                print(f\"  üìä Review user's activity over the past 24 hours\")\n",
    "            else:\n",
    "                print(f\"  üü¢ LOW PRIORITY: Flag for routine review\")\n",
    "                print(f\"  üìù Document pattern for future reference\")\n",
    "    \n",
    "    def display(self):\n",
    "        \"\"\"Display the investigation widget.\"\"\"\n",
    "        return self.widget\n",
    "\n",
    "# Create and display the interactive investigator\n",
    "if profiler.anomalies_data:\n",
    "    investigator = InteractiveAnomalyInvestigator(profiler)\n",
    "    investigator.display()\n",
    "else:\n",
    "    print(\"‚ùå No anomalies to investigate. Try running the analysis first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efbfa57",
   "metadata": {},
   "source": [
    "## üìä Anomaly Detection Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40faeef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_detection_metrics():\n",
    "    \"\"\"Calculate performance metrics for anomaly detection.\"\"\"\n",
    "    if not profiler.events_data:\n",
    "        print(\"‚ùå No events data available\")\n",
    "        return\n",
    "    \n",
    "    total_events = len(profiler.events_data)\n",
    "    detected_anomalies = len(profiler.anomalies_data)\n",
    "    detection_rate = detected_anomalies / total_events\n",
    "    \n",
    "    # Calculate score distribution\n",
    "    all_scores = []\n",
    "    for event, entities in profiler.events_data:\n",
    "        anomaly_scores = profiler.scoring_engine.score_event(event, entities)\n",
    "        aggregate_score = profiler.scoring_engine.calculate_aggregate_score(anomaly_scores)\n",
    "        all_scores.append(aggregate_score)\n",
    "    \n",
    "    all_scores = np.array(all_scores)\n",
    "    \n",
    "    metrics = {\n",
    "        'total_events': total_events,\n",
    "        'detected_anomalies': detected_anomalies,\n",
    "        'detection_rate': detection_rate,\n",
    "        'mean_score': np.mean(all_scores),\n",
    "        'std_score': np.std(all_scores),\n",
    "        'median_score': np.median(all_scores),\n",
    "        'max_score': np.max(all_scores),\n",
    "        'min_score': np.min(all_scores),\n",
    "        'scores_above_threshold': np.sum(all_scores >= profiler.config['anomaly_threshold']),\n",
    "        'score_95th_percentile': np.percentile(all_scores, 95),\n",
    "        'score_99th_percentile': np.percentile(all_scores, 99)\n",
    "    }\n",
    "    \n",
    "    print(\"üìà Anomaly Detection Performance Metrics\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"üìä Dataset Overview:\")\n",
    "    print(f\"  ‚Ä¢ Total events analyzed: {metrics['total_events']:,}\")\n",
    "    print(f\"  ‚Ä¢ Anomalies detected: {metrics['detected_anomalies']:,}\")\n",
    "    print(f\"  ‚Ä¢ Detection rate: {metrics['detection_rate']:.2%}\")\n",
    "    \n",
    "    print(f\"\\nüìä Score Distribution:\")\n",
    "    print(f\"  ‚Ä¢ Mean score: {metrics['mean_score']:.2f}\")\n",
    "    print(f\"  ‚Ä¢ Median score: {metrics['median_score']:.2f}\")\n",
    "    print(f\"  ‚Ä¢ Standard deviation: {metrics['std_score']:.2f}\")\n",
    "    print(f\"  ‚Ä¢ Score range: {metrics['min_score']:.2f} - {metrics['max_score']:.2f}\")\n",
    "    \n",
    "    print(f\"\\nüìà Percentile Analysis:\")\n",
    "    print(f\"  ‚Ä¢ 95th percentile: {metrics['score_95th_percentile']:.2f}\")\n",
    "    print(f\"  ‚Ä¢ 99th percentile: {metrics['score_99th_percentile']:.2f}\")\n",
    "    \n",
    "    print(f\"\\n‚öôÔ∏è Threshold Analysis:\")\n",
    "    print(f\"  ‚Ä¢ Current threshold: {profiler.config['anomaly_threshold']:.1f}\")\n",
    "    print(f\"  ‚Ä¢ Events above threshold: {metrics['scores_above_threshold']:,}\")\n",
    "    \n",
    "    # Quality assessment\n",
    "    print(f\"\\nüíØ Quality Assessment:\")\n",
    "    if metrics['detection_rate'] > 0.10:\n",
    "        print(f\"  ‚ö†Ô∏è High detection rate ({metrics['detection_rate']:.1%}) - consider raising threshold\")\n",
    "    elif metrics['detection_rate'] < 0.01:\n",
    "        print(f\"  ‚ö†Ô∏è Low detection rate ({metrics['detection_rate']:.1%}) - consider lowering threshold\")\n",
    "    else:\n",
    "        print(f\"  ‚úÖ Good detection rate ({metrics['detection_rate']:.1%}) - threshold well calibrated\")\n",
    "    \n",
    "    if metrics['std_score'] < 1.0:\n",
    "        print(f\"  ‚ö†Ô∏è Low score variance - detection may lack sensitivity\")\n",
    "    else:\n",
    "        print(f\"  ‚úÖ Good score variance - detection shows good discrimination\")\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Calculate and display metrics\n",
    "performance_metrics = calculate_detection_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259f8817",
   "metadata": {},
   "source": [
    "## üíæ Export Analysis Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5282fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_analysis_results():\n",
    "    \"\"\"Export analysis results for reporting.\"\"\"\n",
    "    if not profiler.anomalies_data:\n",
    "        print(\"‚ùå No anomalies to export\")\n",
    "        return\n",
    "    \n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    \n",
    "    # Export anomaly summary\n",
    "    anomaly_df = profiler.create_anomaly_summary_df()\n",
    "    anomaly_filename = f'anomaly_analysis_{timestamp}.csv'\n",
    "    anomaly_df.to_csv(anomaly_filename, index=False)\n",
    "    print(f\"üìä Exported anomaly analysis to {anomaly_filename}\")\n",
    "    \n",
    "    # Export detailed results\n",
    "    detailed_results = []\n",
    "    for i, anomaly in enumerate(profiler.anomalies_data):\n",
    "        event = anomaly['event']\n",
    "        \n",
    "        # Extract all factors\n",
    "        all_factors = []\n",
    "        individual_scores = []\n",
    "        \n",
    "        for score in anomaly['anomaly_scores']:\n",
    "            individual_scores.append(score.score)\n",
    "            all_factors.extend(score.factors)\n",
    "        \n",
    "        detailed_results.append({\n",
    "            'anomaly_id': i + 1,\n",
    "            'timestamp': event.eventTime.isoformat(),\n",
    "            'user_arn': event.userIdentity.arn,\n",
    "            'user_name': event.userIdentity.userName,\n",
    "            'event_name': event.eventName,\n",
    "            'source_ip': event.sourceIPAddress,\n",
    "            'aws_region': event.awsRegion,\n",
    "            'user_agent': event.userAgent,\n",
    "            'aggregate_score': anomaly['aggregate_score'],\n",
    "            'max_individual_score': max(individual_scores) if individual_scores else 0,\n",
    "            'num_scoring_factors': len(all_factors),\n",
    "            'scoring_factors': '; '.join(all_factors),\n",
    "            'entities_detected': '; '.join([f\"{e.entity_type}:{e.entity_id}\" for e in anomaly['entities']]),\n",
    "            'hour_of_day': event.eventTime.hour,\n",
    "            'day_of_week': event.eventTime.weekday(),\n",
    "            'is_weekend': event.eventTime.weekday() >= 5,\n",
    "            'is_business_hours': 8 <= event.eventTime.hour <= 18\n",
    "        })\n",
    "    \n",
    "    detailed_df = pd.DataFrame(detailed_results)\n",
    "    detailed_filename = f'detailed_anomalies_{timestamp}.csv'\n",
    "    detailed_df.to_csv(detailed_filename, index=False)\n",
    "    print(f\"üìã Exported detailed anomalies to {detailed_filename}\")\n",
    "    \n",
    "    # Export configuration and metrics\n",
    "    config_and_metrics = {\n",
    "        'analysis_timestamp': datetime.now().isoformat(),\n",
    "        'configuration': profiler.config,\n",
    "        'scoring_engine_summary': profiler.scoring_engine.get_scoring_summary(),\n",
    "        'performance_metrics': performance_metrics if 'performance_metrics' in globals() else {},\n",
    "        'dataset_summary': {\n",
    "            'total_events': len(profiler.events_data),\n",
    "            'total_anomalies': len(profiler.anomalies_data),\n",
    "            'anomaly_rate': len(profiler.anomalies_data) / len(profiler.events_data) if profiler.events_data else 0\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    config_filename = f'analysis_config_{timestamp}.json'\n",
    "    with open(config_filename, 'w') as f:\n",
    "        json.dump(config_and_metrics, f, indent=2, default=str)\n",
    "    print(f\"‚öôÔ∏è Exported configuration and metrics to {config_filename}\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Analysis export complete! Files saved:\")\n",
    "    print(f\"  ‚Ä¢ {anomaly_filename} - Summary analysis\")\n",
    "    print(f\"  ‚Ä¢ {detailed_filename} - Detailed anomaly records\")\n",
    "    print(f\"  ‚Ä¢ {config_filename} - Configuration and metrics\")\n",
    "\n",
    "# Export button\n",
    "export_btn = widgets.Button(description=\"üíæ Export Results\", button_style='success')\n",
    "export_btn.on_click(lambda x: export_analysis_results())\n",
    "display(export_btn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5106ce1",
   "metadata": {},
   "source": [
    "## üéØ Summary and Next Steps\n",
    "\n",
    "### üìä Analysis Summary\n",
    "\n",
    "This notebook provides comprehensive anomaly profiling capabilities for security event analysis:\n",
    "\n",
    "1. **Real-time Anomaly Detection**: Score events using multiple detection algorithms\n",
    "2. **Behavioral Analysis**: Profile user and entity behavior patterns\n",
    "3. **Interactive Investigation**: Drill down into specific anomalies\n",
    "4. **Threshold Optimization**: Find optimal detection thresholds\n",
    "5. **Performance Metrics**: Evaluate detection system performance\n",
    "\n",
    "### üîç Key Insights Available\n",
    "\n",
    "- **User Behavior Patterns**: Identify users with unusual activity\n",
    "- **Time-based Anomalies**: Detect off-hours or unusual timing patterns\n",
    "- **Entity Combinations**: Find rare or suspicious entity relationships\n",
    "- **Privilege Escalation**: Identify potential privilege abuse patterns\n",
    "- **Statistical Outliers**: Detect events that deviate from normal baselines\n",
    "\n",
    "### üõ†Ô∏è Customization Options\n",
    "\n",
    "- **Adjust Detection Thresholds**: Fine-tune sensitivity vs false positive rates\n",
    "- **Configure Anomaly Types**: Enable/disable specific detection algorithms\n",
    "- **Custom Scoring Weights**: Adjust importance of different anomaly types\n",
    "- **Time Window Analysis**: Focus on specific time periods\n",
    "- **Entity-specific Analysis**: Analyze specific users, IPs, or resources\n",
    "\n",
    "### üìà Next Steps for Security Teams\n",
    "\n",
    "1. **Integrate with SIEM**: Connect findings to existing security workflows\n",
    "2. **Create Alerting Rules**: Set up automated alerts for high-scoring anomalies\n",
    "3. **Develop Playbooks**: Create investigation procedures for different anomaly types\n",
    "4. **Train Models**: Use labeled data to improve detection accuracy\n",
    "5. **Regular Reviews**: Schedule periodic analysis to catch evolving threats\n",
    "\n",
    "### üí° Pro Tips\n",
    "\n",
    "- **Start Conservative**: Begin with higher thresholds and adjust based on results\n",
    "- **Focus on Context**: Always consider business context when investigating anomalies\n",
    "- **Track False Positives**: Keep records to improve detection algorithms\n",
    "- **Collaborate**: Share findings with security teams and business stakeholders\n",
    "- **Automate Where Possible**: Use results to build automated response workflows\n",
    "\n",
    "Happy anomaly hunting! üîçüõ°Ô∏è"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
